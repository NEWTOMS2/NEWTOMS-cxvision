{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CxVISION \n",
    "This notebook will guide you on how to deploy the CxVision Model Package from AWS Marketplace to detect and track people in videos. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage instructions\n",
    "\n",
    "Run this Jupyter notebook one cell at a time and press `Shift+Enter` to run each consecutive cell."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "1. This notebook renders correctly in Jupyter Notebook interface, so please open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "2. Ensure that IAM role used has `AmazonSageMakerFullAccess` policy.\n",
    "3. To deploy this ML model successfully, ensure that:\n",
    "   1. You are able to make AWS Marketplace subscriptions and the IAM role has the following permissions:\n",
    "      - `aws-marketplace:ViewSubscriptions`\n",
    "      - `aws-marketplace:Unsubscribe`\n",
    "      - `aws-marketplace:Subscribe`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Subscribe to the model package](#1.-subscribe-to-the-model-package)\n",
    "2. [Create the model](#2.-Create-the-model)\n",
    "3. [Deploy the model](#3.-Deploy-the-model)\n",
    "4. [Perform inferences](#5.-Perform-inferences)\n",
    "5. [Visualize results](#6.-Visualize-results)\n",
    "6. [Clean Up your environment](#7.-Clean-Up-your-environment)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Subscribe to the model package\n",
    "\n",
    "1. Open the model package page [\"Customer experience vision\"]().\n",
    "2. Click on the **Continue to subscribe** button.\n",
    "3. On the **Subscribe to this software** page, review it and click on **\"Accept Offer\"**.\n",
    "4. Click on **Continue to configuration button** and then select a **region**. The **Product ARN** will be displayed. **This is the model package ARN that you need to specify while creating a deployable model using Boto3.**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create the model\n",
    "\n",
    "Create the model using the ARN of the CXVision Model Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import ModelPackage\n",
    "\n",
    "sagemaker_role = '<YourSageMakerRole>' # Replace with your IAM Role for SageMaker\n",
    "sagemaker_model_package_arn = '<ModelPackageARN>' # Replace this with your subcribed model package ARN\n",
    "\n",
    "model = ModelPackage(role=sagemaker_role, \n",
    "                     model_package_arn=sagemaker_model_package_arn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deploy the model\n",
    "The following cell deploys the created model on a real-time endpoint.\n",
    "\n",
    "> Please, replace with your instance type. Supported instances types: ml.g4dn.xlarge, ml.g4dn.2xlarge, ml.g4dn.4xlarge, ml.g4dn.8xlarge, ml.g4dn.12xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "initial_instance_count=1    \n",
    "instance_type='<InstanceType>' # Replace with your instance type. Supported instances types: ml.g4dn.xlarge, ml.g4dn.2xlarge, ml.g4dn.4xlarge, ml.g4dn.8xlarge, ml.g4dn.12xlarge\n",
    "job_name='cxvision'\n",
    "\n",
    "endpoint_name = '{}-{}'.format(job_name,str(uuid.uuid4()))\n",
    "print('SageMaker Endpoint:',endpoint_name)\n",
    "    \n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=initial_instance_count,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "print('Enpoint name: {}'.format(endpoint_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Perform inferences\n",
    "To invoke the endpoint, please ensure your video is in the folder `/videos` and follow the next steps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.0 Preprocess your video\n",
    "For better performance, your video must be prepared for the solution. \n",
    "\n",
    "This process will:\n",
    "1. Rescale your video if the resolution is greater than 1280x72.\n",
    "2. Generate a new video at 1 fps (Frames per second)\n",
    "\n",
    "> Note: if the video to be preprocessed is large, the resources of the instance will likely need to be increased."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video fps: 30\n",
      "Original video Width: 1028 - Video Height: 732\n",
      "0/45\n",
      "1/45\n",
      "2/45\n",
      "3/45\n",
      "4/45\n",
      "5/45\n",
      "6/45\n",
      "7/45\n",
      "8/45\n",
      "9/45\n",
      "10/45\n",
      "11/45\n",
      "12/45\n",
      "13/45\n",
      "14/45\n",
      "15/45\n",
      "16/45\n",
      "17/45\n",
      "18/45\n",
      "19/45\n",
      "20/45\n",
      "21/45\n",
      "22/45\n",
      "23/45\n",
      "24/45\n",
      "25/45\n",
      "26/45\n",
      "27/45\n",
      "28/45\n",
      "29/45\n",
      "30/45\n",
      "31/45\n",
      "32/45\n",
      "33/45\n",
      "34/45\n",
      "35/45\n",
      "36/45\n",
      "37/45\n",
      "38/45\n",
      "39/45\n",
      "40/45\n",
      "41/45\n",
      "42/45\n",
      "43/45\n",
      "44/45\n",
      "45/45\n"
     ]
    }
   ],
   "source": [
    "from utils.preprocess import *\n",
    "\n",
    "video = \"videos/example-video.mp4\"\n",
    "preprocess_video(video)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Real-time inferences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.1 Create the payload for real-time inferences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content type of the required payload for starting to make inferences is `multipart/form-data`. This object has the following attributes:\n",
    "\n",
    "* **video_name**: Indicates the name of the video to be processed. The required format is `<video-name>_area_<area-name>`. Example: `example_area_restaurant`. `area-name` can't have the underscore symbol. \n",
    "* **detection_threshold**: Indicates the threshold to consider a detection as a valid one. All the detections with a confidence equal to or higher than the detection_threshold will be taken. `It must be a float value between 5 and 100.`\n",
    "* **blurring:** `True` if you want to apply blurring on the detected people. Otherwise, its value must be `False`.\n",
    "* **timezone:**  Defines the timezone for processing the videos.\n",
    "* **refresh_threshold:** Indicates the threshold for cleaning the endpoint variables. This value is expressed in hours. By default, its value is 1 (hour).\n",
    "* **dwell_zone:** Defines the dwelling area of the video. Please, see how to define the areas in the [Define Areas Notebook](./utils/DefineAreas.ipynb).\n",
    "* **service_zone:** Defines the service area of the video. Please, see how to define the areas in the [Define Areas Notebook](./utils/DefineAreas.ipynb).\n",
    "* **video**: The video that will be processed by the solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the object according to your data\n",
    "import urllib3\n",
    "\n",
    "video = \"videos/preprocess_example-video.mp4\"\n",
    "\n",
    "payload, content_type = urllib3.encode_multipart_formdata({\n",
    "    \"video_name\": \"example_area_restaurant\",\n",
    "    \"detection_threshold\": \"50\",\n",
    "    \"blurring\": \"True\",\n",
    "    \"timezone\": \"America/Los_Angeles\",\n",
    "    \"refresh_threshold\": \"24\",\n",
    "    \"dwell_zone\": \"[(200,500), (500,510), (490,710), (190,700)]\",\n",
    "    \"service_zone\": \"[(540,580), (1000,645), (990,720), (530,655)]\",\n",
    "    \"video\": (\"video.mp4\", open(video, \"rb\").read(), \"video/mp4\")\n",
    "})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.2 Invoke endpoint\n",
    "\n",
    "The following code will invoke the endpoint to process the video specified in the payload. It would return a JSON object with the detection and tracking information and the generated metrics in form of logs.\n",
    "\n",
    "- Real-time endpoint has a request timeout of 60 seconds.\n",
    "- Input data must be at most 6MB.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "response_file = 'output/response.json'\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "with open(response_file, 'w') as outfile:\n",
    "    outfile.write(json.dumps(json.load(response['Body'])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For an endpoint, limit the maximum size of the input data per invocation to 6 MB. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Perform batch inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the videos you want to process to an S3 bucket and start the batch transform job. For better performance, we recommend preprocessing each video before uploading it by executing the [Preprocess your video](#-4.0-preprocess-your-video)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_path = \"s3://cxvision/batch-process\"\n",
    "s3_output_path = 'YourS3OutputPath'\n",
    "instance_type='InstanceType' # Replace with your instance type. Supported instances types: ml.g4dn.xlarge, ml.g4dn.2xlarge, ml.g4dn.4xlarge, ml.g4dn.8xlarge, ml.g4dn.12xlarge\n",
    "\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    assemble_with=\"None\",\n",
    "    output_path=s3_output_path,\n",
    "    max_concurrent_transforms=1,\n",
    "    max_payload=6\n",
    ")\n",
    "\n",
    "transformer.transform(\n",
    "    data=s3_data_path,\n",
    "    data_type=\"S3Prefix\",\n",
    "    content_type=\"video/mp4\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Currently, batch inference does not support zone capabilities. The detection and tracking will be done for all the people in the video."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate video with tracking results\n",
    "The endpoint response contains the coordinates of the detections (tracking attribute) and the metrics in the form of logs (resume attribute). \n",
    "\n",
    "Example: [Model response](./sample/output/output-model-sample.json)\n",
    "\n",
    "With the following cell, you will generate the final video by drawing bounding boxes in each video frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Blurring people (if necessary)\n",
      "{'y0': 269, 'y1': 659, 'x0': 326, 'x1': 439}\n",
      "{'y0': 279, 'y1': 650, 'x0': 644, 'x1': 769}\n",
      "{'y0': 357, 'y1': 690, 'x0': 946, 'x1': 1027}\n",
      "{'y0': 418, 'y1': 720, 'x0': 364, 'x1': 478}\n",
      "{'y0': 220, 'y1': 423, 'x0': 784, 'x1': 865}\n",
      "{'y0': 231, 'y1': 353, 'x0': 776, 'x1': 892}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 266, 'y1': 644, 'x0': 349, 'x1': 463}\n",
      "{'y0': 279, 'y1': 652, 'x0': 645, 'x1': 771}\n",
      "{'y0': 357, 'y1': 691, 'x0': 946, 'x1': 1027}\n",
      "{'y0': 412, 'y1': 715, 'x0': 388, 'x1': 501}\n",
      "{'y0': 227, 'y1': 428, 'x0': 783, 'x1': 868}\n",
      "{'y0': 231, 'y1': 353, 'x0': 775, 'x1': 892}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 264, 'y1': 642, 'x0': 357, 'x1': 471}\n",
      "{'y0': 275, 'y1': 649, 'x0': 650, 'x1': 778}\n",
      "{'y0': 343, 'y1': 691, 'x0': 946, 'x1': 1027}\n",
      "{'y0': 392, 'y1': 700, 'x0': 411, 'x1': 526}\n",
      "{'y0': 221, 'y1': 416, 'x0': 779, 'x1': 862}\n",
      "{'y0': 227, 'y1': 356, 'x0': 773, 'x1': 894}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 245, 'y1': 625, 'x0': 401, 'x1': 513}\n",
      "{'y0': 273, 'y1': 652, 'x0': 647, 'x1': 777}\n",
      "{'y0': 353, 'y1': 691, 'x0': 947, 'x1': 1027}\n",
      "{'y0': 397, 'y1': 700, 'x0': 434, 'x1': 548}\n",
      "{'y0': 228, 'y1': 424, 'x0': 786, 'x1': 872}\n",
      "{'y0': 227, 'y1': 357, 'x0': 773, 'x1': 894}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 252, 'y1': 643, 'x0': 384, 'x1': 501}\n",
      "{'y0': 272, 'y1': 651, 'x0': 650, 'x1': 781}\n",
      "{'y0': 351, 'y1': 693, 'x0': 947, 'x1': 1027}\n",
      "{'y0': 389, 'y1': 695, 'x0': 445, 'x1': 560}\n",
      "{'y0': 227, 'y1': 420, 'x0': 789, 'x1': 876}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 251, 'y1': 638, 'x0': 335, 'x1': 451}\n",
      "{'y0': 272, 'y1': 647, 'x0': 643, 'x1': 770}\n",
      "{'y0': 353, 'y1': 692, 'x0': 946, 'x1': 1027}\n",
      "{'y0': 387, 'y1': 695, 'x0': 449, 'x1': 564}\n",
      "{'y0': 225, 'y1': 413, 'x0': 789, 'x1': 874}\n",
      "{'y0': 228, 'y1': 352, 'x0': 783, 'x1': 898}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 248, 'y1': 634, 'x0': 331, 'x1': 453}\n",
      "{'y0': 278, 'y1': 648, 'x0': 659, 'x1': 788}\n",
      "{'y0': 355, 'y1': 694, 'x0': 945, 'x1': 1027}\n",
      "{'y0': 388, 'y1': 695, 'x0': 438, 'x1': 554}\n",
      "{'y0': 228, 'y1': 352, 'x0': 783, 'x1': 898}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 249, 'y1': 638, 'x0': 339, 'x1': 466}\n",
      "{'y0': 274, 'y1': 645, 'x0': 666, 'x1': 795}\n",
      "{'y0': 353, 'y1': 693, 'x0': 944, 'x1': 1027}\n",
      "{'y0': 390, 'y1': 694, 'x0': 432, 'x1': 548}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 254, 'y1': 633, 'x0': 345, 'x1': 474}\n",
      "{'y0': 271, 'y1': 645, 'x0': 668, 'x1': 799}\n",
      "{'y0': 359, 'y1': 693, 'x0': 951, 'x1': 1027}\n",
      "{'y0': 388, 'y1': 696, 'x0': 434, 'x1': 552}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 256, 'y1': 660, 'x0': 343, 'x1': 481}\n",
      "{'y0': 271, 'y1': 647, 'x0': 669, 'x1': 800}\n",
      "{'y0': 360, 'y1': 694, 'x0': 953, 'x1': 1027}\n",
      "{'y0': 386, 'y1': 695, 'x0': 439, 'x1': 557}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 255, 'y1': 661, 'x0': 346, 'x1': 485}\n",
      "{'y0': 267, 'y1': 645, 'x0': 664, 'x1': 797}\n",
      "{'y0': 377, 'y1': 629, 'x0': 276, 'x1': 372}\n",
      "{'y0': 233, 'y1': 347, 'x0': 751, 'x1': 855}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 268, 'y1': 644, 'x0': 663, 'x1': 795}\n",
      "{'y0': 379, 'y1': 628, 'x0': 258, 'x1': 353}\n",
      "{'y0': 231, 'y1': 347, 'x0': 772, 'x1': 876}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 272, 'y1': 643, 'x0': 673, 'x1': 804}\n",
      "{'y0': 226, 'y1': 344, 'x0': 775, 'x1': 880}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 276, 'y1': 642, 'x0': 675, 'x1': 806}\n",
      "{'y0': 227, 'y1': 343, 'x0': 777, 'x1': 882}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 279, 'y1': 645, 'x0': 671, 'x1': 801}\n",
      "{'y0': 227, 'y1': 346, 'x0': 769, 'x1': 874}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 277, 'y1': 644, 'x0': 674, 'x1': 805}\n",
      "{'y0': 226, 'y1': 344, 'x0': 772, 'x1': 875}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 283, 'y1': 650, 'x0': 670, 'x1': 802}\n",
      "{'y0': 229, 'y1': 349, 'x0': 770, 'x1': 875}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 283, 'y1': 649, 'x0': 670, 'x1': 802}\n",
      "{'y0': 230, 'y1': 350, 'x0': 769, 'x1': 873}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 284, 'y1': 649, 'x0': 670, 'x1': 802}\n",
      "{'y0': 230, 'y1': 355, 'x0': 767, 'x1': 874}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 285, 'y1': 652, 'x0': 669, 'x1': 801}\n",
      "{'y0': 231, 'y1': 353, 'x0': 768, 'x1': 873}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 282, 'y1': 648, 'x0': 669, 'x1': 802}\n",
      "{'y0': 510, 'y1': 729, 'x0': 954, 'x1': 1014}\n",
      "{'y0': 229, 'y1': 348, 'x0': 770, 'x1': 873}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 282, 'y1': 650, 'x0': 666, 'x1': 800}\n",
      "{'y0': 551, 'y1': 731, 'x0': 968, 'x1': 1019}\n",
      "{'y0': 475, 'y1': 728, 'x0': 780, 'x1': 884}\n",
      "{'y0': 230, 'y1': 349, 'x0': 768, 'x1': 871}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 435, 'y1': 726, 'x0': 626, 'x1': 731}\n",
      "{'y0': 283, 'y1': 645, 'x0': 668, 'x1': 800}\n",
      "{'y0': 565, 'y1': 731, 'x0': 970, 'x1': 1018}\n",
      "{'y0': 469, 'y1': 730, 'x0': 835, 'x1': 946}\n",
      "{'y0': 228, 'y1': 378, 'x0': 784, 'x1': 859}\n",
      "{'y0': 230, 'y1': 349, 'x0': 768, 'x1': 870}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 446, 'y1': 731, 'x0': 644, 'x1': 747}\n",
      "{'y0': 277, 'y1': 667, 'x0': 664, 'x1': 807}\n",
      "{'y0': 473, 'y1': 731, 'x0': 862, 'x1': 972}\n",
      "{'y0': 239, 'y1': 362, 'x0': 804, 'x1': 890}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n",
      "Blurring people (if necessary)\n",
      "{'y0': 273, 'y1': 655, 'x0': 655, 'x1': 794}\n",
      "{'y0': 235, 'y1': 354, 'x0': 790, 'x1': 878}\n",
      "Drawing bounding boxes\n",
      "Drawing tracking resume\n"
     ]
    }
   ],
   "source": [
    "from utils.show_results import *\n",
    "\n",
    "output_video = \"output/video-results-1.mp4\"\n",
    "generate_video(video, response_file, output_video)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Clean Up your environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1. Delete model and endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(endpoint_name)\n",
    "model.delete_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2. Unsubscribe (optional)\n",
    "\n",
    "1. Ensure that you do not have a [running model](https://console.aws.amazon.com/sagemaker/home#/models).\n",
    "2. Go to __Machine Learning__ tab on [Your Software Subscriptions](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust) page.\n",
    "2. Locate the listing that you want to cancel the subscription for and then choose __Cancel Subscription__."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (tags/v3.8.10:3d8993a, May  3 2021, 11:48:03) [MSC v.1928 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "74fbae03f09acb54b4b85bc5f11519af67119ba3a464556648626baa681cdd43"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
