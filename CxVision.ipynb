{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CxVISION \n",
    "This notebook will guide you in how to deploy the CxVision Model Package from AWS Marketplace to detect and track people in videos. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Usage instructions\n",
    "\n",
    "Run this Jupyter notebook one cell a time and press `Shift+Enter` to run each consecutive cell."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-requisites\n",
    "\n",
    "1. This notebook renders correctly in Jupyter Notebook interface, so please open this notebook from an Amazon SageMaker Notebook Instance or Amazon SageMaker Studio.\n",
    "2. Ensure that IAM role used has `AmazonSageMakerFullAccess` policy.\n",
    "3. To deploy this ML model successfully, ensure that:\n",
    "   1. You are able to make AWS Marketplace subscriptions and the IAM role has the following permissions:\n",
    "      - `aws-marketplace:ViewSubscriptions`\n",
    "      - `aws-marketplace:Unsubscribe`\n",
    "      - `aws-marketplace:Subscribe`\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "\n",
    "1. [Subscribe to the model package](#1.-Subscribe-to-the-model-package)\n",
    "2. [Create the model](#2.-Create-the-model)\n",
    "3. [Deploy the model](#3.-Deploy-the-model)\n",
    "4. [Perform inferences](#5.-Perform-inferences)\n",
    "5. [Visualize results](#6.-Visualize-results)\n",
    "6. [Clean Up your environment](#7.-Clean-Up-your-environment)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Subscribe to the model package\n",
    "\n",
    "1. Open the model package page [\"Customer experience vision\"]().\n",
    "2. Click on the **Continue to subscribe** button.\n",
    "3. On the **Subscribe to this software** page, review it and click on **\"Accept Offer\"**.\n",
    "4. Click on **Continue to configuration button** and then select a **region**. The **Product ARN** will be displayed. **This is the model package ARN that you need to specify while creating a deployable model using Boto3.**\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Create the model\n",
    "\n",
    "Create the model using the ARN of the CXVision Model Package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import ModelPackage\n",
    "\n",
    "sagemaker_role = '<YourSageMakerRole>' # Replace with your IAM Role for SageMaker\n",
    "sagemaker_model_package_arn = '<ModelPackageARN>' # Replace this with your subcribed model package ARN\n",
    "\n",
    "model = ModelPackage(role=sagemaker_role, \n",
    "                     model_package_arn=sagemaker_model_package_arn)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Deploy the model\n",
    "The following cell deploys the created model on a real-time endpoint.\n",
    "\n",
    "> Please, replace with your instance type. Supported instances types: ml.g4dn.xlarge, ml.g4dn.2xlarge, ml.g4dn.4xlarge, ml.g4dn.8xlarge, ml.g4dn.12xlarge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "\n",
    "initial_instance_count=1    \n",
    "instance_type='<InstanceType>' # Replace with your instance type. Supported instances types: ml.g4dn.xlarge, ml.g4dn.2xlarge, ml.g4dn.4xlarge, ml.g4dn.8xlarge, ml.g4dn.12xlarge\n",
    "job_name='cxvision'\n",
    "\n",
    "endpoint_name = '{}-{}'.format(job_name,str(uuid.uuid4()))\n",
    "print('SageMaker Endpoint:',endpoint_name)\n",
    "    \n",
    "predictor = model.deploy(\n",
    "    initial_instance_count=initial_instance_count,\n",
    "    instance_type=instance_type,\n",
    "    endpoint_name=endpoint_name\n",
    ")\n",
    "\n",
    "print('Enpoint name: {}'.format(endpoint_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Perform inferences\n",
    "To invoke the endpoint, please ensure your video is in the folder `/videos` and follow the next steps."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.0 Preprocess your video\n",
    "\n",
    "For better performance, your video must be prepared for the solution. \n",
    "\n",
    "The following cell will: \n",
    "1. Rescale the video if the video resolution is greater than 1280x72.\n",
    "2. Generate a new video at 1 fps (Frame per second)\n",
    "\n",
    "> Note: if the video to be preprocessed is very large, it is likely that the resources of the instance will need to be increased so that it can be fully processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.preprocess import *\n",
    "\n",
    "video = \"videos/example-video.mp4\"\n",
    "preprocess_video(video)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.1 Real-time inferences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.1 Create the payload for real-time inferences"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The content type of the required payload for start making inferences is `multipart/form-data`. This object has the following attributes:\n",
    "\n",
    "* **detection_threshold**: Indicates the threshold to consider a detection as valid one. All the detections with the confidence equal or higher than the detection_threshold will be taken. `It must be a float value between 5 and 100.`\n",
    "* **blurring:** `True` if you want to apply blurring on the detected people. Otherwise, it's value must be `False`.\n",
    "* **timezone:**  Defines the timezone for processing the videos.\n",
    "* **refresh_threshold:** Indicates the threshold for cleaning the endpoint variables. This value is express in hours. By default, its value is 24 (hours).\n",
    "* **dwell_zone:** Defines the dwell area of the video. Please, see how to define the areas in the [Define Areas Notebook](./utils/DefineAreas.ipynb).\n",
    "* **service_zone:** Defines the service area of the video. Please, see how to define the areas in the [Define Areas Notebook](./utils/DefineAreas.ipynb).\n",
    "* **video**: The video to be processed by the solution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the object according to your data\n",
    "import urllib3\n",
    "\n",
    "video = \"videos/preprocess_example-video.mp4\"\n",
    "\n",
    "payload, content_type = urllib3.encode_multipart_formdata({\n",
    "    \"detection_threshold\": \"50\",\n",
    "    \"blurring\": \"True\",\n",
    "    \"timezone\": \"America/Los_Angeles\",\n",
    "    \"refresh_threshold\": \"24\",\n",
    "    \"dwell_zone\": \"[(200,500), (500,510), (490,710), (190,700)]\",\n",
    "    \"service_zone\": \"[(540,580), (1000,645), (990,720), (530,655)]\",\n",
    "    \"video\": (\"video.mp4\", open(video, \"rb\").read(), \"video/mp4\")\n",
    "})\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 4.1.2 Invoke endpoint\n",
    "\n",
    "The following code will invoke the endpoint to  process the video specified in the payload. It would return an JSON object with the detection and tracking information and the generated metrics in form of logs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import json\n",
    "\n",
    "sm_runtime = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "response_file = 'output/response.json'\n",
    "response = sm_runtime.invoke_endpoint(\n",
    "    EndpointName=endpoint_name,\n",
    "    ContentType=content_type,\n",
    "    Body=payload\n",
    ")\n",
    "\n",
    "with open(response_file, 'w') as outfile:\n",
    "    outfile.write(json.dumps(json.load(response['Body'])))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> For an endpoint, limit the maximum size of the input data per invocation to 6 MB. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Perform batch inference"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the videos you want to process to an S3 bucket and the create the batch transform job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_data_path = \"s3://cxvision/batch-process\"\n",
    "s3_output_path = 'YourS3OutputPath'\n",
    "instance_type='InstanceType' # Replace with your instance type. Supported instances types: ml.g4dn.xlarge, ml.g4dn.2xlarge, ml.g4dn.4xlarge, ml.g4dn.8xlarge, ml.g4dn.12xlarge\n",
    "\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    assemble_with=\"None\",\n",
    "    output_path=s3_output_path,\n",
    "    max_concurrent_transforms=1,\n",
    "    max_payload=6\n",
    ")\n",
    "\n",
    "transformer.transform(\n",
    "    data=s3_data_path,\n",
    "    data_type=\"S3Prefix\",\n",
    "    content_type=\"video/mp4\"\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Batch inference does not support zones capabilities. The detection and tracking will be done for all the people in the video."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate video with tracking results\n",
    "The endpoint response contains the coordinates of the detections (tracking attribute) and the metrics in the form of logs (resume attribute). \n",
    "\n",
    "Example: [Model response](./sample/output/output-model-sample.json)\n",
    "\n",
    "With the following cell you will generate the final video by drawing bouding boxes in each video frame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.show_results import *\n",
    "\n",
    "output_video = \"output/video-results.mp4\"\n",
    "generate_video(video, response_file, output_video)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Clean Up your environment"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.1. Delete model and endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.sagemaker_session.delete_endpoint(endpoint_name)\n",
    "model.delete_model()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7.2. Unsubscribe (optional)\n",
    "\n",
    "1. Ensure that you do not have a [running model](https://console.aws.amazon.com/sagemaker/home#/models).\n",
    "2. Go to __Machine Learning__ tab on [Your Software Subscriptions](https://aws.amazon.com/marketplace/ai/library?productType=ml&ref_=mlmp_gitdemo_indust) page.\n",
    "2. Locate the listing that you want to cancel the subscription for and then choose __Cancel Subscription__."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2b7c52bd8ad02206de645265cab3458653dd72da6782bfda53779bacacfdc4b4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
